# Install necessary packages first (if not already installed)
# You can uncomment and run this block in your local environment
# !pip install stable-baselines3[extra] gym pandas numpy

import gym
from gym import spaces
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env

# Load preprocessed data
KBdf = pd.read_csv("/mnt/data/KBdf_prepared.csv")
PUdf = pd.read_csv("/mnt/data/PUdf_prepared.csv")
HOFdf = pd.read_csv("/mnt/data/HOFdf_prepared.csv")
PHdf = pd.read_csv("/mnt/data/PHdf_prepared.csv")
Tstyles = pd.read_csv("/mnt/data/Tstyles_prepared.csv").squeeze()

# Define Gym Environment
class TranslationBTSSGymEnv(gym.Env):
    def __init__(self, KBdf, PUdf, HOFdf, PHdf, Tstyles):
        super(TranslationBTSSGymEnv, self).__init__()
        self.KBdf = KBdf
        self.PUdf = PUdf
        self.HOFdf = HOFdf
        self.PHdf = PHdf
        self.Tstyles = Tstyles
        self.current_step = 0
        self.max_steps = len(KBdf)

        # Define observation and action spaces
        self.observation_space = spaces.Dict({
            "KB": spaces.Box(low=0, high=10, shape=(3,), dtype=np.float32),
            "PU": spaces.Box(low=0, high=10, shape=(2,), dtype=np.float32),
            "HOF": spaces.Discrete(3),
            "PH": spaces.Discrete(3),
        })
        self.actions = ["insert", "delete", "pause_short", "pause_long", "gaze_ST", "gaze_TT", "commit_PU"]
        self.action_space = spaces.Discrete(len(self.actions))

        self.target_pub_duration = {
            'rapid': 1.0,
            'cautious': 3.0,
            'balanced': 2.0,
            'deliberate': 4.0,
            'hesitant': 5.0
        }

    def reset(self):
        self.current_step = 0
        return self._get_obs()

    def step(self, action):
        obs = self._get_obs()
        style = self.Tstyles[self.current_step]
        target_duration = self.target_pub_duration[style]
        reward = -abs(self.PUdf.iloc[self.current_step]["Duration"] - target_duration)
        self.current_step += 1
        done = self.current_step >= self.max_steps
        return obs, reward, done, {}

    def _get_obs(self):
        return {
            "KB": self.KBdf.iloc[self.current_step].values.astype(np.float32),
            "PU": self.PUdf.iloc[self.current_step].values.astype(np.float32),
            "HOF": int(self.HOFdf.iloc[self.current_step]),
            "PH": int(self.PHdf.iloc[self.current_step])
        }

# Create environment instance
env = TranslationBTSSGymEnv(KBdf, PUdf, HOFdf, PHdf, Tstyles)

# Flatten observation space for training with SB3
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.preprocessing import flatten_dict_observation

class FlattenWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        flat_space = spaces.flatten_space(env.observation_space)
        self.observation_space = flat_space

    def observation(self, observation):
        return spaces.flatten(self.env.observation_space, observation)

flat_env = FlattenWrapper(env)

# Train PPO agent
model = PPO("MlpPolicy", flat_env, verbose=1)
model.learn(total_timesteps=5000)

# Save model
model.save("/mnt/data/ppo_translation_btss")

